# Policy Under Pressure

**Interactive experiments exploring how intelligent agents behave when rewards, sensors, and physics turn hostile**

ğŸ”— **[Live Demo](https://your-deployment-url.vercel.app)** | ğŸ“Š **Research Project** | ğŸ“ **September 2025 - January 2026**

![Policy Under Pressure](https://img.shields.io/badge/Status-Complete-success?style=for-the-badge)
![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=for-the-badge&logo=typescript&logoColor=white)
![React](https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)

---

## ğŸ¯ Overview

This project systematically compares **Reinforcement Learning (RL)** and **Evolution Strategies (ES)** through adversarial testing in a 2D racing environment. We expose both methods to hostile conditionsâ€”reward misalignment, sensor corruption, and physics manipulationâ€”to reveal fundamental differences in their learned behaviors.

### Key Findings

- âœ… **ES wins on robustness**: Superior performance under all adversarial conditions
- âš ï¸ **RL exhibits policy collapse**: Learned to minimize speed as a survival strategy
- ğŸ“Š **Reward misalignment matters**: Small changes in reward functions drastically alter behavior
- ğŸ”¬ **Gradient-based vs Population-based**: Fundamental differences in optimization landscapes

---

## ğŸ“‚ Project Structure

```
d:/agent compare project/
â”œâ”€â”€ frontend/                    # React + Vite web application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/              # 4 segment pages + homepage
â”‚   â”‚   â”œâ”€â”€ components/         # Reusable UI components
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ public/                 # Pre-generated JSON experiment results
â”‚       â”œâ”€â”€ experiment_results.json
â”‚       â”œâ”€â”€ segment2_results_v2.json
â”‚       â”œâ”€â”€ gradient_results_v2.json
â”‚       â””â”€â”€ segment4_*.json (4 files)
â”‚
â”œâ”€â”€ backend/                    # Python physics/reward engine
â”‚   â”œâ”€â”€ physics/               # Car dynamics, friction, noise
â”‚   â”œâ”€â”€ rewards/               # Reward function definitions
â”‚   â””â”€â”€ experiments/           # Wrappers for adversarial testing
â”‚
â”œâ”€â”€ models/                    # Trained agent checkpoints (.pkl, .zip)
â”œâ”€â”€ train_*.py                 # Training scripts for all agents
â”œâ”€â”€ run_*.py                   # Experiment execution scripts
â””â”€â”€ experiments.md             # Detailed methodology notes

```

---

## ğŸš€ Quick Start

### Prerequisites

- **Node.js** 18+ (for frontend)
- **Python** 3.9+ (for training/experiments - optional)

### Running Locally

```bash
# Clone the repository
git clone https://github.com/yourusername/policy-under-pressure.git
cd policy-under-pressure

# Install frontend dependencies
cd frontend
npm install

# Start development server
npm run dev
```

The app will be available at `http://localhost:8080`

### Building for Production

```bash
cd frontend
npm run build
# Output in frontend/dist/
```

---

## ğŸŒ Deployment

### Deploy to Vercel (Recommended)

1. **Install Vercel CLI**:
   ```bash
   npm i -g vercel
   ```

2. **Deploy**:
   ```bash
   vercel
   ```

3. **Follow prompts** - Vercel auto-detects the configuration from `vercel.json`

### Manual Deployment

Deploy the `frontend/dist/` folder to any static hosting:
- **Netlify**: Drag & drop `dist` folder
- **GitHub Pages**: Push `dist` to `gh-pages` branch
- **Cloudflare Pages**: Connect repo and set build command

---

## ğŸ“Š Experiments Overview

### Segment 1: Fragility
**Baseline comparison** - Both agents tested on standard track (Figure-8)
- RL achieves higher speed but exhibits brittleness
- ES shows more conservative, stable behavior

### Segment 2: Robustness
**Adversarial physics** - Friction reduction and sensor noise injection
- **Friction Ladder**: RL collapses to near-zero speed, ES maintains function
- **Noise Injection**: ES degrades gracefully, RL already at floor performance

### Segment 3: Gradient Analysis  
**Statistical robustness** - Aggregated metrics across severity levels
- RL stuck in local minimum (policy collapse during training)
- ES benefits from population-based exploration
- Reveals fundamental differences in optimization landscapes

### Segment 4: Reward Manipulation
**Controlled experiments** - Systematic reward engineering
1. **Exploit Behavior**: Both agents show preference for easier tracks
2. **Sensitivity Analysis**: Â±10%, Â±20% reward shifts expose brittleness
3. **Misalignment Test**: RL maintains quality despite wrong rewards (learned representations)
4. **Parallel Scaling**: ES scales linearly, RL plateaus

---

## ğŸ”¬ Technical Details

### Agents

**Reinforcement Learning (RL - PPO)**
- **Algorithm**: Proximal Policy Optimization
- **Framework**: Stable-Baselines3
- **Architecture**: 2-layer MLP (64 units)
- **Training**: 500K timesteps, gradient-based optimization
- **Observation**: 8D state vector (position, velocity, orientation, track)
- **Action**: 2D continuous (steering, throttle)

**Evolution Strategies (ES)**
- **Algorithm**: OpenAI ES (CMA-ES variant)
- **Framework**: Custom implementation
- **Architecture**: 2-layer MLP (64 units, matching RL)
- **Training**: 300 generations, population size 50
- **Fitness**: Episodic reward (no gradients)

### Environment

- **Track**: Figure-8 layout (2 loops, sharp turns)
- **Physics**: 2D car dynamics with friction, momentum
- **Adversarial Modes**:
  - Friction reduction (Î¼: 1.0 â†’ 0.2)
  - Sensor noise (Gaussian, Ïƒ: 0.0 â†’ 1.0)
  - Reward delay (0-20 timesteps)
  - Action masking (random dropout)

### Data Pipeline

All experiments were run **once** and results saved as JSON:
1. **Training**: `train_*.py` scripts â†’ save models to `/models/`
2. **Experiments**: `run_*.py` scripts â†’ load models, run tests, save JSON
3. **Frontend**: React app fetches pre-generated JSON, no server needed

---

## ğŸ“ˆ Key Metrics

- **Speed**: Mean velocity (m/s) during episode
- **Lateral Error**: RMS distance from track centerline
- **Survival Rate**: % of episodes completing without crash
- **Steering Variance**: Control activity indicator

---

## ğŸ› ï¸ Tech Stack

### Frontend
- **React** 18.3 + **TypeScript**
- **Vite** 5.4 (build tool)
- **TailwindCSS** 3.4 (styling)
- **Recharts** 2.15 (data visualization)
- **Framer Motion** (animations)
- **Lucide React** (icons)

### Backend (Training Only)
- **Python** 3.9+
- **PyTorch** 2.0+
- **Stable-Baselines3** (RL)
- **NumPy**, **Matplotlib** (data/viz)

---

## ğŸ“ Citation

If you use this work in your research, please cite:

```bibtex
@misc{policyunderpressure2026,
  title={Policy Under Pressure: Adversarial Testing of RL vs ES},
  author={Keshav Majithia},
  year={2026},
  howpublished={\url{https://your-deployment-url.vercel.app}}
}
```

---

## ğŸ¤ Contributing

This is a completed research project, but feedback and discussions are welcome!

1. **Issues**: Report bugs or suggest improvements
2. **Discussions**: Technical questions about methodology
3. **Forks**: Feel free to extend with your own experiments

---

## ğŸ“„ License

MIT License - see `LICENSE` file for details

---

## ğŸ‘¤ Author

**Keshav Majithia**
- ğŸ”— [LinkedIn](https://linkedin.com/in/keshav-m-9a2701252)
- ğŸ¦ [Twitter](https://twitter.com/keshav_m__)
- ğŸ“§ [Email](mailto:keshavmajithia13@gmail.com)

---

## ğŸ™ Acknowledgments

- **OpenAI** - Evolution Strategies inspiration
- **Stable-Baselines3** - RL implementation
- **Recharts** - Beautiful charts
- **Vercel** - Hosting platform

---

**Built with â¤ï¸ over 5 months** | September 2025 - January 2026

â­ **Star this repo** if you found it interesting!
