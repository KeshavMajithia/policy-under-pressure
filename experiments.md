Phase 2: Behavioral Comparison Experiments

(All agents already know how to drive)

Important rule:
No experiment is allowed to re-teach driving.
We only stress, distort, or restrict what already exists.

1ï¸âƒ£ Reward Exploitation (Cheating)

Setup

Add a tiny loophole reward (e.g. +0.1 for lateral oscillation, boundary grazing, lap reset timing)

Question

Who discovers non-driving behaviors faster?

Expected

RL: precise exploit, sharp policy collapse

ES: ignores unless exploit is globally stable

Metric

Reward vs actual progress divergence

2ï¸âƒ£ Environment Generalization

Setup

Train on Track A

Test on:

unseen curvature

mirrored tracks

randomized widths

Question

Who learned â€œdrivingâ€ vs â€œthis trackâ€?

Expected

ES generalizes better due to invariant geometry

RL overfits curvature statistics

3ï¸âƒ£ Observation Noise

Setup

Inject noise into:

lateral error

heading error

speed

Question

Who maintains control under sensor uncertainty?

Metric

Steering smoothness

Track exit rate

4ï¸âƒ£ Reward Delay & Corruption

Setup

Delay reward by N steps

Randomly drop reward 30% of the time

Insight
ES doesnâ€™t care when reward arrives â€” RL does.

5ï¸âƒ£ Sample Efficiency

Setup

Same environment

Same architecture

Compare reward vs environment steps

Interpretation
Not â€œwho winsâ€ â€” but how expensive intelligence is.

6ï¸âƒ£ Training Stability

Setup

10 random seeds each

Track collapses, regressions

Expected

RL: faster rise, occasional collapse

ES: boring, monotonic improvement

7ï¸âƒ£ Behavioral Diversity

Setup

Log trajectories across seeds

Metric

Entropy of action distribution

Path variance

Interpretation
Exploration style, not performance.

8ï¸âƒ£ Reward Sensitivity

Setup

Change penalty coefficients by Â±10%

Question

Who breaks when the rules slightly change?

9ï¸âƒ£ Alignment with True Objective

Setup

Intentionally misalign reward

Observe behavior visually

This is your â€œAI alignmentâ€ experiment
Very strong paper angle.

ğŸ”Ÿ Graceful Degradation

Setup

Smaller networks

Less compute

Fewer steps

Question

Who still drives like a car instead of a glitch?

1ï¸âƒ£1ï¸âƒ£ Interpretability of Failure

Setup

Force failure cases

Analyze why the policy broke

ES failures are often smooth and explainable.

1ï¸âƒ£2ï¸âƒ£ Compute Scaling

Setup

Same wall-clock

ES with more CPUs

RL with larger batch sizes

1ï¸âƒ£3ï¸âƒ£ Partial Observability

Setup

Remove heading info

Remove speed

Question

Who infers missing state better?

1ï¸âƒ£4ï¸âƒ£ Overfitting to Physics Quirks

Setup

Slightly change friction / mass

Expected

RL breaks earlier

ES tolerates drift

1ï¸âƒ£5ï¸âƒ£ Natural Motion

Setup

Human evaluation or smoothness metrics

This is your visual + qualitative killer result.

4ï¸âƒ£ Final verdict (important)

You are not comparing Usain Bolt vs a newborn.

You are comparing:

A sharp optimizer vs a robust optimizer
A gradient thinker vs a population thinker

And Phase 1 did exactly what it needed to:

teach both how to drive

remove survival nonsense

remove reward confusion

enforce discipline

5ï¸âƒ£ What you should do next (concrete)

Do NOT code yet.

Next steps:

Freeze Phase 1 (no more tuning)

Write Phase 2 Experiment Protocols (like above)

Decide which 5â€“6 experiments become the paper core

Then build frontend visualizations only for those


1ï¸âƒ£ Reward Exploitation (Cheating Behavior)
What you test

Introduce a reward that can be exploited:

Example: high reward for speed, weak penalty for corner deviation

What you measure

Reward â†‘ vs track deviation â†‘

Steering saturation

Corner cutting frequency

Expected reality

RL: exploits aggressively (rides edges, cuts corners)

ES: ignores exploit unless globally safe

Interpretation

RL finds loopholes faster
ES resists reward hacking but sacrifices performance

ğŸ“Œ This is not failure â€” itâ€™s value alignment vs optimization power.

2ï¸âƒ£ Generalization Across Environment Changes
What you change

New unseen tracks

Slightly altered curvature

Different straight/turn ratios

What you measure

Speed drop

Off-track events

Recovery time

Expected

RL: sharp performance drop, then partial recovery

ES: slower but stable immediately

ğŸ“Œ RL memorizes how to win
ğŸ“Œ ES learns how not to die

3ï¸âƒ£ Robustness to Observation Noise
What you change

Gaussian noise on position/heading

Partial sensor dropout

Metrics

Steering oscillation

Lane deviation

Crash probability

Expected

RL: twitchy, oscillatory

ES: damped, smooth

ğŸ“Œ ES wins here â€” not because itâ€™s smart, but because itâ€™s cautious.

4ï¸âƒ£ Robustness to Reward Noise & Delay
What you change

Delayed reward

Sparse rewards

Random reward masking

Expected

RL: destabilizes (credit assignment hell)

ES: largely unaffected

ğŸ“Œ ES doesnâ€™t care when reward comes
ğŸ“Œ RL absolutely does

5ï¸âƒ£ Sample Efficiency
What you measure

Reward vs environment steps

Time to first clean lap

Expected

RL dominates

ES lags badly

ğŸ“Œ This is not controversial.
ğŸ“Œ This is why RL is used in robotics, games, control.

6ï¸âƒ£ Stability of Training
What you test

Multiple seeds

Long training runs

Metrics

Reward variance

Sudden collapses

Expected

RL: sharp gains, occasional collapse

ES: boring, monotonic

ğŸ“Œ ESâ€™s â€œboringâ€ behavior is a feature.

7ï¸âƒ£ Behavioral Diversity
What you measure

Trajectory variance

Speed profiles

Steering entropy

Expected

ES explores policy space globally

RL converges to a single dominant behavior

ğŸ“Œ ES = population thinker
ğŸ“Œ RL = winner-takes-all thinker

8ï¸âƒ£ Sensitivity to Reward Shaping
What you change

Slight reward coefficient tweaks

Expected

RL: behavior shifts dramatically

ES: relatively unchanged

ğŸ“Œ RL is fragile to reward design
ğŸ“Œ ES is reward-robust but conservative

9ï¸âƒ£ Alignment with True Objective
True objective

â€œDrive cleanly, smoothly, and correctly â€” even if reward is imperfectâ€

Expected

ES behaves â€œcorrectlyâ€ even with bad rewards

RL follows reward literally

ğŸ“Œ This is your alignment experiment

ğŸ”Ÿ Graceful Degradation
What you change

Smaller networks

Fewer training steps

Reduced compute

Expected

RL collapses suddenly

ES degrades smoothly

ğŸ“Œ ES fails gracefully
ğŸ“Œ RL fails catastrophically

1ï¸âƒ£1ï¸âƒ£ Interpretability of Failure
What you analyze

Why did the agent fail?

Expected

ES failures are simple: â€œtoo slowâ€, â€œtoo cautiousâ€

RL failures are chaotic: oscillation, oversteering, reward chasing

ğŸ“Œ ES is easier to reason about.

1ï¸âƒ£2ï¸âƒ£ Compute Scaling Behavior
What you change

CPUs / parallel rollouts

Expected

ES scales linearly

RL hits diminishing returns

ğŸ“Œ This is ESâ€™s biggest strength historically.

1ï¸âƒ£3ï¸âƒ£ Memory & Partial Observability
What you change

Remove heading

Mask future curvature

Expected

ES unaffected (trajectory-level optimization)

RL struggles (Markov violation)

ğŸ“Œ ES doesnâ€™t need perfect state.

1ï¸âƒ£4ï¸âƒ£ Overfitting to Environment Quirks
What you test

Remove invisible shortcuts

Slight track randomization

Expected

RL overfits quirks

ES learns invariant behavior

ğŸ“Œ RL learns tricks
ğŸ“Œ ES learns principles

1ï¸âƒ£5ï¸âƒ£ Emergence of Natural Motion
What you observe

Smoothness

Human-like driving

Expected

ES looks human

RL looks optimal but robotic

ğŸ“Œ This is huge for real-world systems.