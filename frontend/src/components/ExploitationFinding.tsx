import { AlertTriangle, TrendingDown, TrendingUp } from 'lucide-react';

export const ExploitationFinding = () => {
    return (
        <div className="bg-card/30 backdrop-blur-sm rounded-xl border border-muted p-4 mt-6">
            <div className="flex items-center gap-2 mb-4">
                <AlertTriangle className="w-4 h-4 text-orange-500" />
                <h3 className="text-xs font-mono text-muted-foreground uppercase tracking-wider">
                    Research Finding: Reward Exploitation
                </h3>
            </div>

            <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
                <div className="p-4 bg-background/40 rounded-lg border border-white/5">
                    <h4 className="text-sm font-bold text-rl mb-2 flex items-center gap-2">
                        RL Adjustment <TrendingDown className="w-4 h-4" />
                    </h4>
                    <div className="space-y-2">
                        <div className="flex justify-between text-xs font-mono">
                            <span className="text-muted-foreground">Standard Speed:</span>
                            <span>~5.0 m/s</span>
                        </div>
                        <div className="flex justify-between text-xs font-mono">
                            <span className="text-muted-foreground">Cheating Speed:</span>
                            <span className="text-orange-400">~1.5 m/s</span>
                        </div>
                    </div>
                    <p className="mt-3 text-xs text-muted-foreground leading-relaxed">
                        When incentivized for purely "speed" without crash penalties, RL paradoxically <strong>slowed down</strong>.
                        It discovered that driving slowly minimizes crash risk, allowing it to survive longer and accumulate more total reward than a risky fast run.
                    </p>
                </div>

                <div className="p-4 bg-background/40 rounded-lg border border-white/5">
                    <h4 className="text-sm font-bold text-es mb-2 flex items-center gap-2">
                        ES Response <TrendingUp className="w-4 h-4 ml-auto" />
                    </h4>
                    <div className="space-y-2">
                        <div className="flex justify-between text-xs font-mono">
                            <span className="text-muted-foreground">Standard Speed:</span>
                            <span>~5.0 m/s</span>
                        </div>
                        <div className="flex justify-between text-xs font-mono">
                            <span className="text-muted-foreground">Cheating Speed:</span>
                            <span>~2.2 m/s</span>
                        </div>
                    </div>
                    <p className="mt-3 text-xs text-muted-foreground leading-relaxed">
                        ES also slowed down but maintained a slightly higher pace. This confirms that both algorithms are susceptible to
                        <strong> Reward Hacking</strong>, optimizing for the mathematical function rather than the designer's intent.
                    </p>
                </div>
            </div>

            <div className="mt-4 p-3 border-l-2 border-orange-500 bg-orange-500/10">
                <p className="text-xs text-orange-200">
                    <strong>Conclusion:</strong> The "Cheating Test" reveals that without explicit constraints (Safety Penalties), agents will find the "Path of Least Resistance" - often producing boring or unexpected behavior (The "Turtle Strategy") to maximize rewards.
                </p>
            </div>
        </div>
    );
};
